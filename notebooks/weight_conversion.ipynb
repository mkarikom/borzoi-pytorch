{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3104b665-faa5-44fa-b689-8584ad0f1912",
   "metadata": {},
   "source": [
    "# Borzoi weight conversion from TF to Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "084764e4-3fef-4d05-8d99-061b51049c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import h5py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.config.experimental.enable_tensor_float_32_execution(False)\n",
    "import baskerville\n",
    "from baskerville import seqnn\n",
    "import json\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a65b119c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_the_transformer(layers):\n",
    "    weights = dict()\n",
    "    last_layer = \"\"\n",
    "    transformer_layer = 0\n",
    "    prefix = f\"transformer.{transformer_layer}\"\n",
    "    for layer_name, layer in layers.items():\n",
    "        prefix = f\"transformer.{transformer_layer}\"\n",
    "        if \"batch_normalization_6\" in layer_name:\n",
    "            break\n",
    "        if (\n",
    "            \"layer_normalization\" in layer_name\n",
    "            and \"multihead\" not in last_layer\n",
    "        ):\n",
    "            weights[f\"{prefix}.0.fn.0.weight\"] = layer[0]\n",
    "            weights[f\"{prefix}.0.fn.0.bias\"] = layer[1]\n",
    "        if \"multihead\" in layer_name:\n",
    "            weights[f\"{prefix}.0.fn.1.rel_content_bias\"] = layer[0]\n",
    "            weights[f\"{prefix}.0.fn.1.rel_pos_bias\"] = layer[1]\n",
    "            weights[f\"{prefix}.0.fn.1.to_q.weight\"] = layer[2].T\n",
    "            weights[f\"{prefix}.0.fn.1.to_k.weight\"] = layer[3].T\n",
    "            weights[f\"{prefix}.0.fn.1.to_v.weight\"] = layer[4].T\n",
    "            weights[f\"{prefix}.0.fn.1.to_out.weight\"] = layer[5].T\n",
    "            weights[f\"{prefix}.0.fn.1.to_out.bias\"] = layer[6]\n",
    "            weights[f\"{prefix}.0.fn.1.to_rel_k.weight\"] = layer[7].T\n",
    "        if \"layer_normalization\" in layer_name and \"multihead\" in last_layer:\n",
    "            weights[f\"{prefix}.1.fn.0.weight\"] = layer[0]\n",
    "            weights[f\"{prefix}.1.fn.0.bias\"] = layer[1]\n",
    "        if \"dense\" in layer_name and \"dense\" not in last_layer:\n",
    "            weights[f\"{prefix}.1.fn.1.weight\"] = layer[0].T\n",
    "            weights[f\"{prefix}.1.fn.1.bias\"] = layer[1]\n",
    "        if \"dense\" in layer_name and \"dense\" in last_layer:\n",
    "            weights[f\"{prefix}.1.fn.4.weight\"] = layer[0].T\n",
    "            weights[f\"{prefix}.1.fn.4.bias\"] = layer[1]\n",
    "            transformer_layer += 1\n",
    "        last_layer = layer_name\n",
    "    return weights\n",
    "\n",
    "\n",
    "def convert_the_convs(layers):\n",
    "    weights = dict()\n",
    "    conv_lookup = {\n",
    "        \"conv1d\": \"conv_dna.conv_layer\",\n",
    "        \"conv1d_1\": \"res_tower.0.conv_layer\",\n",
    "        \"conv1d_2\": \"res_tower.2.conv_layer\",\n",
    "        \"conv1d_3\": \"res_tower.4.conv_layer\",\n",
    "        \"conv1d_4\": \"res_tower.6.conv_layer\",\n",
    "        \"conv1d_5\": \"res_tower.8.conv_layer\",\n",
    "        \"conv1d_6\": \"unet1.1.conv_layer\",\n",
    "        \"separable_conv1d\": \"separable1.conv_layer\",\n",
    "        \"separable_conv1d_1\": \"separable0.conv_layer\",\n",
    "        \"dense_16\": \"upsampling_unet1.0.conv_layer\",\n",
    "        \"dense_17\": \"horizontal_conv1.conv_layer\",\n",
    "        \"dense_18\": \"upsampling_unet0.0.conv_layer\",\n",
    "        \"dense_19\": \"horizontal_conv0.conv_layer\",\n",
    "        \"conv1d_7\": \"final_joined_convs.0.conv_layer\",\n",
    "        \"dense_20\": \"human_head\",\n",
    "    }\n",
    "    for conv_tf, conv_pt in conv_lookup.items():\n",
    "        if \"separable\" in conv_tf:\n",
    "            weights[f\"{conv_pt}.0.weight\"] = layers[conv_tf][0].permute(\n",
    "                (1, 2, 0)\n",
    "            )\n",
    "            weights[f\"{conv_pt}.1.weight\"] = layers[conv_tf][1].permute(\n",
    "                (2, 1, 0)\n",
    "            )\n",
    "            weights[f\"{conv_pt}.1.bias\"] = layers[conv_tf][2]\n",
    "        else:\n",
    "            try:\n",
    "                weights[f\"{conv_pt}.weight\"] = layers[conv_tf][0].permute(\n",
    "                    (2, 1, 0)\n",
    "                )\n",
    "            except:\n",
    "                weights[f\"{conv_pt}.weight\"] = (\n",
    "                    layers[conv_tf][0].unsqueeze(0).permute((2, 1, 0))\n",
    "                )\n",
    "            weights[f\"{conv_pt}.bias\"] = layers[conv_tf][1]\n",
    "    return weights\n",
    "\n",
    "\n",
    "def normalize_the_norms(layers):\n",
    "    weights = dict()\n",
    "    norm_lookup = {\n",
    "        \"batch_normalization\": \"res_tower.0.norm\",\n",
    "        \"batch_normalization_1\": \"res_tower.2.norm\",\n",
    "        \"batch_normalization_2\": \"res_tower.4.norm\",\n",
    "        \"batch_normalization_3\": \"res_tower.6.norm\",\n",
    "        \"batch_normalization_4\": \"res_tower.8.norm\",\n",
    "        \"batch_normalization_5\": \"unet1.1.norm\",\n",
    "        \"batch_normalization_6\": \"upsampling_unet1.0.norm\",\n",
    "        \"batch_normalization_7\": \"horizontal_conv1.norm\",\n",
    "        \"batch_normalization_8\": \"upsampling_unet0.0.norm\",\n",
    "        \"batch_normalization_9\": \"horizontal_conv0.norm\",\n",
    "        \"batch_normalization_10\": \"final_joined_convs.0.norm\",\n",
    "    }\n",
    "    for norm_tf, norm_pt in norm_lookup.items():\n",
    "        weights[f\"{norm_pt}.weight\"] = layers[norm_tf][0]\n",
    "        weights[f\"{norm_pt}.bias\"] = layers[norm_tf][1]\n",
    "        weights[f\"{norm_pt}.running_mean\"] = layers[norm_tf][2]\n",
    "        weights[f\"{norm_pt}.running_var\"] = layers[norm_tf][3]\n",
    "    return weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc393b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Specify the path pattern\n",
    "path_pattern = \"/nfs/turbo/umms-welchjd/mkarikom/borzoi_tf_weights/model0_best.h5*\"\n",
    "\n",
    "# Get list of all files matching the pattern\n",
    "model_file_list = glob.glob(path_pattern)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd0c265d-adae-4771-8ea5-0cd27ac9db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_file = \"/nfs/turbo/umms-welchjd/mkarikom/borzoi_tf_weights/params_pred.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab97ad2-29c0-4cee-babd-f17bb1e82c22",
   "metadata": {},
   "source": [
    "We transfer each checkpoint individually, by constructing the keras model from the checkpoint and then saving all weights to a dictionary.<br> We then translate the dictionary to a pytorch state_dict where keys match the current architecture. There are easier ways to do this (from the weights.h5 without having TF installed)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ca312c2-0372-42cb-bca8-9614317f2d4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Take\n",
    "with open(params_file) as params_open :\n",
    "    params = json.load(params_open)\n",
    "    params_model = params['model']\n",
    "    params_train = params['train']\n",
    "    \n",
    "params_model['verbose'] = True\n",
    "\n",
    "seqnn_model = seqnn.SeqNN(params_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e7134b3-e2a1-46a8-89f4-1fea9dc9edc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get model weights from keras model\n",
    "\n",
    "torch_weights_path = \"/nfs/turbo/umms-welchjd/mkarikom/borzoi_torch_weights\"\n",
    "\n",
    "for model_file in model_file_list:\n",
    "    seqnn_model.restore(model_file, 0)\n",
    "    layer_weight_dict = dict()\n",
    "    for layer in seqnn_model.model.layers: \n",
    "        cfg = layer.get_config()\n",
    "        weights = layer.get_weights()\n",
    "        if len(weights) != 0:\n",
    "            layer_weight_dict[cfg['name']] = weights\n",
    "            \n",
    "    assert \"batch_normalization\" in layer_weight_dict.keys()\n",
    "    # If this fails, it probably means you have to restart the kernel as each call of seqnn.SeqNN(params_model) increases the layernames numbers\n",
    "\n",
    "    sorted(layer_weight_dict.keys())\n",
    "\n",
    "    layer_weight_dict['conv_dna.conv_layer'] = layer_weight_dict['conv1d']\n",
    "    for key, layer in layer_weight_dict.items():\n",
    "        temp_list = []\n",
    "        for weights in layer:\n",
    "            temp_list.append(torch.as_tensor(weights))\n",
    "        layer_weight_dict[key] = temp_list\n",
    "        \n",
    "        \n",
    "    res_transformers = transform_the_transformer(layer_weight_dict)\n",
    "    res_convs = convert_the_convs(layer_weight_dict)\n",
    "    res_norms = normalize_the_norms(layer_weight_dict)\n",
    "\n",
    "\n",
    "    z = {**res_transformers, **res_convs, **res_norms}\n",
    "\n",
    "    torch.save(z, f\"{torch_weights_path}/{model_file.split('/')[-1]}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
